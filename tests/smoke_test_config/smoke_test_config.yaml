## ------------------------------------------------------------------------- ##
## --------------------- Integration Tests Main Config --------------------- ##
## ------------------------------------------------------------------------- ##

# Smoke Test Model
MODEL_DIR: "openai-community/gpt2"

# Prompt
PROMPT: "Copyright (C"

# Layers Slice to Cache Activations From
ACTS_LAYERS_SLICE: "7:9"

# Topk thresholds for gradient-based method.
NUM_DOWN_NODES: 10
NUM_UP_NODES: 10

# Autoencoder Size
PROJECTION_FACTOR: 32

# Autoencoder Save Files
ENCODER_FILE: "smoke_test_encoder.pt"
ENC_BIASES_FILE: "smoke_test_enc_biases.pt"
DECODER_FILE: "smoke_test_decoder.pt"
DEC_BIASES_FILE: "smoke_test_dec_biases.pt"
ATTN_ENCODER_FILE: "attn_encoder.pt"
ATTN_ENC_BIASES_FILE: "attn_enc_biases.pt"
ATTN_DECODER_FILE: "attn_decoder.pt"
ATTN_DEC_BIASES_FILE: "attn_dec_biases.pt"
MLP_ENCODER_FILE: "mlp_encoder.pt"
MLP_ENC_BIASES_FILE: "mlp_enc_biases.pt"
MLP_DECODER_FILE: "mlp_decoder.pt"
MLP_DEC_BIASES_FILE: "mlp_dec_biases.pt"


# Smoke Test Save Files
PROMPT_IDS_FILE: "smoke_test_activations_prompt_ids.npy"
ACTS_DATA_FILE: "smoke_test_activations_dataset.pt"
ATTN_DATA_FILE: "attn_acts_dataset.pt"
MLP_DATA_FILE: "mlp_acts_dataset.pt"
TOP_K_INFO_FILE: "smoke_test_token_info.csv"
ATTN_TOKEN_FILE: "attn_tokens.csv"
MLP_TOKEN_FILE: "mlp_tokens.csv"
GRAPH_FILE: "smoke_test_graph.svg"
GRAPH_DOT_FILE: "smoke_test_graph.dot"
GRADS_FILE: "grads_graph.svg"
GRADS_DOT_FILE: "grads_graph.dot"

# Autoencoder Training
LAMBDA_L1: 1e-4
LEARNING_RATE: 1.0e-4
TRAINING_BATCH_SIZE: 8
NUM_WORKERS: 0
ACCUMULATE_GRAD_BATCHES: 1

# Cognition Graph
COEFFICIENT: 0.0
BRANCHING_FACTOR: 5
INIT_THINNING_FACTOR: 0.2
LOGIT_TOKENS: 8
THRESHOLD_EXP: 5.01
DIMS_PINNED:
  7: [15755]

## ------------------------------------------------------------------------- ##
## ------------------------- Stable Test Constants ------------------------- ##
## ------------------------------------------------------------------------- ##

# Reproducibility
SEED: 0

# `collect_acts.py`
NUM_SEQUENCES_EVALED: 5
MAX_SEQ_LEN: 5

# `train_autoencoder.py`
LOG_EVERY_N_STEPS: 1
EPOCHS: 2
SYNC_DIST_LOGGING: True

# `contexts.py`
DIMS_IN_BATCH: 500
TOP_K: 6

# `cognition_graph.py`
NUM_SEQUENCES_INTERPED: 1
MAX_SEQ_INTERPED_LEN: 10

# `cognition_graph_mc.py`
NUM_QUESTIONS_INTERPED: 1
MAX_NEW_TOKENS: 1
NUM_RETURN_SEQUENCES: 1
NUM_SHOT: 6
