# Model
# "EleutherAI/pythia-70m"
# "meta-llama/Llama-2-7b-hf"
# "meta-llama/Llama-2-70b-hf"
# "rasp"
MODEL_DIR: "rasp"

# Slice of Layers to Cache Activations From
# Put a Python slice here, in str format. Put ":" to use data from all model
# layers.
ACTS_LAYERS_SLICE: "0:2"

# Large Model Mode
# Set True for `Llama-2 7B` and larger, False for smaller. This solves a
# devicing bug in `accelerate` that occurs with smaller models.
LARGE_MODEL_MODE: False

# Encoder Size
PROJECTION_FACTOR: 10

# Save Files
PROMPT_IDS_FILE: "activations_prompt_ids.npy"
ACTS_DATA_FILE: "activations_dataset.pt"
ENCODER_FILE: "learned_encoder.pt"
BIASES_FILE: "learned_biases.pt"
TOP_K_INFO_FILE: "token_info.csv"

# Autoencoder Training
LAMBDA_L1: 1e-2
LEARNING_RATE: 1e-2
TRAINING_BATCH_SIZE: 8
NUM_WORKERS: 8
ACCUMULATE_GRAD_BATCHES: 4

# Reproducibility
SEED: 0


# Stable Dev Constants for `collect_acts.py`
MAX_NEW_TOKENS: 1
NUM_RETURN_SEQUENCES: 1
NUM_SHOT: 6
NUM_QUESTIONS_EVALED: 817

# Stable Dev Constants for `train_autoencoder.py`
LOG_EVERY_N_STEPS: 5
EPOCHS: 150
SYNC_DIST_LOGGING: True

# Stable Dev Constants for `interp_top_tokens.py`
# _Leave out entries_ for None: None values are unfortunately interpreted as
# the string "None"
DIMS_IN_BATCH: 500
TOP_K: 6
